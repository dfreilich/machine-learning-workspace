{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 18739 Security and Fairness of Deep learning\n",
    "## Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Before You Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.2 Loading in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Note: You don't have to use the starter code\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import time\n",
    "def load_data(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    try:\n",
    "        #Python3\n",
    "        train_set, valid_set, test_set = pkl.load(f, encoding='latin1')\n",
    "    except:\n",
    "        #Python2\n",
    "        train_set, valid_set, test_set = pkl.load(f)\n",
    "    f.close()\n",
    "    return(train_set,valid_set,test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(50000, 10)\n",
      "(10000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Loading in the data\n",
    "path = 'mnist.pkl.gz' \n",
    "train_set,valid_set,test_set = load_data(path)\n",
    "print(test_set[0].shape)\n",
    "\n",
    "##preprocess the data\n",
    "y_train = np_utils.to_categorical(train_set[1], 10)\n",
    "y_val = np_utils.to_categorical(valid_set[1],10)\n",
    "y_test = np_utils.to_categorical(test_set[1],10)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Level 1: Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.5777 - acc: 0.8525 - val_loss: 0.3654 - val_acc: 0.9018\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.3766 - acc: 0.8959 - val_loss: 0.3245 - val_acc: 0.9112\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.3449 - acc: 0.9032 - val_loss: 0.3084 - val_acc: 0.9134\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.3284 - acc: 0.9085 - val_loss: 0.2977 - val_acc: 0.9180\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.3176 - acc: 0.9107 - val_loss: 0.2933 - val_acc: 0.9178\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.3100 - acc: 0.9131 - val_loss: 0.2869 - val_acc: 0.9203\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.3044 - acc: 0.9151 - val_loss: 0.2832 - val_acc: 0.9201\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.2994 - acc: 0.9164 - val_loss: 0.2792 - val_acc: 0.9216\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.2956 - acc: 0.9176 - val_loss: 0.2771 - val_acc: 0.9217\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.2923 - acc: 0.9181 - val_loss: 0.2762 - val_acc: 0.9223\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.2896 - acc: 0.9196 - val_loss: 0.2745 - val_acc: 0.9220\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.2870 - acc: 0.9198 - val_loss: 0.2721 - val_acc: 0.9234\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.2849 - acc: 0.9206 - val_loss: 0.2712 - val_acc: 0.9235\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.2830 - acc: 0.9208 - val_loss: 0.2699 - val_acc: 0.9245\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.2812 - acc: 0.9211 - val_loss: 0.2692 - val_acc: 0.9243\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.2797 - acc: 0.9220 - val_loss: 0.2684 - val_acc: 0.9242\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.2780 - acc: 0.9226 - val_loss: 0.2687 - val_acc: 0.9243\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.2768 - acc: 0.9221 - val_loss: 0.2665 - val_acc: 0.9256\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.2756 - acc: 0.9232 - val_loss: 0.2651 - val_acc: 0.9256\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.2741 - acc: 0.9232 - val_loss: 0.2653 - val_acc: 0.9263\n",
      "10000/10000 [==============================] - 0s 9us/step\n",
      "[0.27401491284370422, 0.92320000290870663]\n",
      "Run Time for Training: 15.958046913146973\n",
      "Test accuracy: 0.923200002909\n"
     ]
    }
   ],
   "source": [
    "## Start your code here\n",
    "\n",
    "##Initialize Parameters\n",
    "batch = 100\n",
    "epochs = 20\n",
    "l_r = 0.1\n",
    "features = 784\n",
    "classes = 10\n",
    "\n",
    "##Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "##add a layer to the model, with a specified score/activation function\n",
    "model.add(Dense(10, activation='softmax', input_shape=(features,)))\n",
    "\n",
    "##Compile your model with the desired loss function, optimizer and metrics\n",
    "#make optimizer, to use learning rate\n",
    "sgd = optimizers.SGD(lr=l_r, decay=0.0, momentum=0.0, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "##Fit your training data (You can also specify your validation data using the validation set here)\n",
    "start_time = time.time()\n",
    "model.fit(train_set[0], y_train, epochs=epochs, batch_size=batch, verbose=2, validation_data=(valid_set[0], y_val))\n",
    "end_time = time.time()\n",
    "\n",
    "# ##Predict on the test data and report the test accuracy(the percentage of images correctly predicted)\n",
    "loss_and_metrics = model.evaluate(test_set[0], y_test, batch_size=batch)\n",
    "print(loss_and_metrics)\n",
    "\n",
    "print('Run Time for Training:', end_time-start_time)\n",
    "print('Test accuracy:', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Report: \n",
    "- Parameters you chose(batch_size,learning rate,epoch): (batch_size=100, learning rate = 0.1, epoch = 20)\n",
    "- Test Accuracy: 0.921900003552\n",
    "- Run Time for Training: 17.713045120239258 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Level 2: Theano Implenmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Accuracy: 0.94982\n",
      "Epoch:  2 Accuracy: 0.96274\n",
      "Epoch:  3 Accuracy: 0.96758\n",
      "Epoch:  4 Accuracy: 0.97012\n",
      "Epoch:  5 Accuracy: 0.97174\n",
      "Epoch:  6 Accuracy: 0.9734\n",
      "Epoch:  7 Accuracy: 0.97446\n",
      "Epoch:  8 Accuracy: 0.97514\n",
      "Epoch:  9 Accuracy: 0.9759\n",
      "Epoch:  10 Accuracy: 0.97626\n",
      "Epoch:  11 Accuracy: 0.97666\n",
      "Epoch:  12 Accuracy: 0.97704\n",
      "Epoch:  13 Accuracy: 0.97744\n",
      "Epoch:  14 Accuracy: 0.97768\n",
      "Epoch:  15 Accuracy: 0.97808\n",
      "Epoch:  16 Accuracy: 0.97836\n",
      "Epoch:  17 Accuracy: 0.97868\n",
      "Epoch:  18 Accuracy: 0.97888\n",
      "Epoch:  19 Accuracy: 0.97916\n",
      "Epoch:  20 Accuracy: 0.97932\n",
      "Run Time for Training 2.2022199630737305\n",
      "Test accuracy: 0.97932\n"
     ]
    }
   ],
   "source": [
    "## Start your code here\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "#Note: You don't have to use the starter code\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_input, n_output):\n",
    "        \"\"\" Initialize the parameters(W,b,p(Y|X)...) of the logistic regression\n",
    "        :X The training/testing set input\n",
    "        :m,n: dimension of training set X\n",
    "        \"\"\"\n",
    "        ## initialize the weights and bias term\n",
    "        self.W = theano.shared(\n",
    "            value=np.random.randn(n_input, n_output),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_output,)),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input\n",
    "        \n",
    "        ## initialize funcs\n",
    "        self.p_y = T.nnet.softmax(T.dot(self.input, self.W) + self.b)\n",
    "        self.pred = T.argmax(self.p_y, axis=1)\n",
    "        self.one_hot = T.extra_ops.to_one_hot(self.pred, n_output)\n",
    "\n",
    "    def score(self, y_true):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "        :y The training set output (ground truth)\n",
    "         \n",
    "        \"\"\"\n",
    "        return T.mean(T.nnet.categorical_crossentropy(self.p_y, y_true))\n",
    "    \n",
    "    def test_accuracy(self, test_y):\n",
    "        \"\"\"Return the test accuracy \n",
    "        :test_y: the testing set output(ground truth)\n",
    "        \"\"\"\n",
    "        ## using self.one_hot, and not self.pred, bc test_y is going to be one_hot encoded as well\n",
    "        return T.mean(T.eq(self.one_hot, test_y))\n",
    "        pass\n",
    "\n",
    "def main():\n",
    "    ##Initialize symbolic variables;\n",
    "    #     np_x_train = np.array(train_set[0])    \n",
    "    x_train_t = theano.shared(np.asarray(train_set[0], dtype=theano.config.floatX))\n",
    "    y_train_t = theano.shared(np.asarray(y_train, dtype=theano.config.floatX))\n",
    "    x_test_t = theano.shared(np.asarray(test_set[0], dtype=theano.config.floatX))\n",
    "    y_test_t = theano.shared(np.asarray(y_test, dtype=theano.config.floatX))\n",
    "\n",
    "    #compute number of minibatches\n",
    "    n_train_batches = x_train_t.get_value(borrow=True).shape[0] // batch\n",
    "    n_test_batches = x_test_t.get_value(borrow=True).shape[0] // batch\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.dmatrix('x')\n",
    "    y = T.dmatrix('y') ## should this be a matrix? We'll see\n",
    "    \n",
    "    ## Create Logistic Regression object and define cost function\n",
    "    LogReg = LogisticRegression(x, features, classes)\n",
    "    cost = LogReg.score(y)\n",
    "\n",
    "    ##Define gradients and update rules;\n",
    "    #this link suggested adding a sum to it: https://groups.google.com/forum/#!msg/theano-users/o1vzlmwVVG8/IjZlwt-GH2AJ\n",
    "    g_W = T.grad(cost=cost, wrt=LogReg.W)\n",
    "    g_b = T.grad(cost=cost, wrt=LogReg.b)\n",
    "    \n",
    "    ##Define your training functions;        \n",
    "    updates = [(LogReg.W, LogReg.W - l_r * g_W),\n",
    "               (LogReg.b, LogReg.b - l_r * g_b)]\n",
    "\n",
    "    train_model = theano.function(\n",
    "              inputs=[index],\n",
    "              outputs=cost,\n",
    "              updates = updates,\n",
    "              givens={\n",
    "                x: x_train_t[index * batch: (index + 1) * batch],\n",
    "                y: y_train_t[index * batch: (index + 1) * batch]\n",
    "            }\n",
    "    )\n",
    "    \n",
    "    test_accuracy = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=LogReg.test_accuracy(y),\n",
    "        givens={\n",
    "            x: x_test_t[index * batch: (index + 1) * batch],\n",
    "            y: y_test_t[index * batch: (index + 1) * batch]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ##Train your model\n",
    "    test_score = 0.\n",
    "    epoch = 0\n",
    "    start_time= time.time()\n",
    "    while (epoch < epochs):\n",
    "        epoch = epoch+1\n",
    "        for batch_index in range(n_train_batches):\n",
    "            avg_cost = train_model(batch_index)\n",
    "        test_losses = [test_accuracy(i)\n",
    "            for i in range(n_test_batches)]\n",
    "        test_score = np.mean(test_losses)\n",
    "        print('Epoch: ', epoch, 'Accuracy:', test_score)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    ##Report your accuracy on the test set.\n",
    "    test_losses = [test_accuracy(i)\n",
    "                   for i in range(n_test_batches)]\n",
    "    test_score = np.mean(test_losses)\n",
    "\n",
    "    print('Run Time for Training', end_time-start_time)\n",
    "    print('Test accuracy:', test_score)\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Report: \n",
    "- Parameters you chose(batch_size,learning rate,epoch): (batch_size=100, learning rate = 0.1, epoch = 20)\n",
    "- Test Accuracy: 0.9786\n",
    "- Run Time for Training: 2.1274356842041016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Level 3: Numpy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Start your code here\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Note: You don't have to use the starter code\n",
    "def softmax(o):\n",
    "    \"\"\"\n",
    "    Returns softmax(o) (dim:10 * n)\n",
    "    :o = WX + b,(dim: 10 * n)\n",
    "    \"\"\"\n",
    "    num = np.exp(o-np.max(o))\n",
    "    if num.ndim == 1:\n",
    "        return num / np.sum(num, axis=0)\n",
    "    else:  \n",
    "        return num / np.array([np.sum(num, axis=1)]).T  # ndim = 2\n",
    "\n",
    "def sgd(n_epoch,lr,W,X,y,batch_size=100):\n",
    "    \"\"\"\n",
    "    Returns output: optimal W,b Stochastic Gradient Descent\n",
    "    :n_epoch: Number of epochs\n",
    "    :lr: learning rate\n",
    "    :W: (dim: 10*785)\n",
    "    :X: (dim: n*784)\n",
    "    :y: (dim: 10*n)   \n",
    "    \"\"\"\n",
    "    X = np.c_[np.ones((X.shape[0])), X]\n",
    "    for epoch in range(n_epoch):\n",
    "#         epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        #calculate batch\n",
    "        for (x_batch, y_batch) in next_batch(X,y,batch_size):\n",
    "            ##Compute predictions, softmax(WX_b)\n",
    "            y_prob = softmax(np.dot(x_batch, W.T))\n",
    "            \n",
    "            ##computes derivative of loss wrt W and b\n",
    "            d_y = (y_prob-y_batch)\n",
    "\n",
    "            ##updates weights based on derivative\n",
    "            W -= lr * ((np.dot(d_y.T, x_batch))/batch_size)\n",
    "            \n",
    "#             epoch_loss.append(cost(y_batch, y_prob))\n",
    "            epoch_acc.append(compute_acc(y_batch, y_prob))\n",
    "#         print('Epoch:', epoch+1, 'Average Accuracy:', np.mean(epoch_acc))\n",
    "    return W\n",
    "\n",
    "def one_hot_encode(seq, num_classes):\n",
    "    num_frames = len(seq)\n",
    "    m = np.zeros((num_frames, num_classes))\n",
    "    m[np.arange(num_frames), seq] = 1\n",
    "    return m\n",
    "\n",
    "def next_batch(X, y, batch_size):\n",
    "     ##loop over our dataset `X` in mini-batches of size `batchSize`\n",
    "    for i in np.arange(0, X.shape[0], batch_size):\n",
    "        ## yield a tuple of the current batched data and labels\n",
    "        yield (X[i:i + batch_size], y[i:i + batch_size])\n",
    "        \n",
    "def cross_entropy(y_true, y_pred):\n",
    "    ent = -np.sum(np.log(y_pred) * (y_true), axis=1)\n",
    "#     print('entropy', ent)\n",
    "    return ent\n",
    "\n",
    "def cost(y_true, y_pred):\n",
    "    c = np.mean(cross_entropy(y_true, y_pred))\n",
    "#     print('cost', c)\n",
    "    return c\n",
    "\n",
    "def compute_acc(y_true, y_prob):\n",
    "    y_pred = np.argmax(y_prob,axis=1)\n",
    "    #transform true labels back to number encoding, to allow more logical equality check\n",
    "    y_true_num = [np.where(r==1)[0][0] for r in y_true]\n",
    "    accuracy = np.sum(y_true_num == y_pred)/y_true.shape[0]\n",
    "    return accuracy * 100\n",
    "    \n",
    "def test_accuracy(X_test,y_test,W):\n",
    "    \"\"\"\n",
    "    Return the accuracy using W,b returned from sgd()\n",
    "    :X_test: (dim: n*785) test dataset input\n",
    "    :y_test: (dim: 10*n) test dataset output\n",
    "    \"\"\"\n",
    "    X_test = np.c_[np.ones((X_test.shape[0])), X_test]\n",
    "    prob = softmax(np.dot(X_test, W.T))\n",
    "    return compute_acc(y_test, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Parameter(Learning Rate) Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Learning Rate: 1e-10\n",
      "Learning Rate: 1e-10 Validation Accuracy: 13.67\n",
      "Evaluating Learning Rate: 1e-09\n",
      "Learning Rate: 1e-09 Validation Accuracy: 13.67\n",
      "Evaluating Learning Rate: 1e-08\n",
      "Learning Rate: 1e-08 Validation Accuracy: 13.67\n",
      "Evaluating Learning Rate: 1e-07\n",
      "Learning Rate: 1e-07 Validation Accuracy: 13.68\n",
      "Evaluating Learning Rate: 1e-06\n",
      "Learning Rate: 1e-06 Validation Accuracy: 13.72\n",
      "Evaluating Learning Rate: 1e-05\n",
      "Learning Rate: 1e-05 Validation Accuracy: 14.19\n",
      "Evaluating Learning Rate: 0.0001\n",
      "Learning Rate: 0.0001 Validation Accuracy: 19.82\n",
      "Evaluating Learning Rate: 0.001\n",
      "Learning Rate: 0.001 Validation Accuracy: 52.68\n",
      "Evaluating Learning Rate: 0.01\n",
      "Learning Rate: 0.01 Validation Accuracy: 82.55\n",
      "Evaluating Learning Rate: 0.1\n",
      "Learning Rate: 0.1 Validation Accuracy: 89.89\n",
      "Evaluating Learning Rate: 1.0\n",
      "Learning Rate: 1.0 Validation Accuracy: 91.85\n",
      "Evaluating Learning Rate: 10.0\n",
      "Learning Rate: 10.0 Validation Accuracy: 88.06\n",
      "Best Learning Rate: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEjCAYAAAA/ugbCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117029550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.9\n",
      "Run Time for Training: 5.516980886459351\n"
     ]
    }
   ],
   "source": [
    "## Start your code here\n",
    "#Note: You don't have to use the starter code\n",
    "def main():\n",
    "    ##Parameter(Learning Rate) Tuning\n",
    "    #Define Parameters and Variables\n",
    "    lrs = np.power(10,np.arange(-10,2),dtype=float)\n",
    "    W = np.random.randn(10,785)\n",
    "    all_valid_acc = []\n",
    "    best_acc = 0\n",
    "    best_lr = 0\n",
    "    best_lr_test = 0\n",
    "    run_time = 0\n",
    "    \n",
    "    for i in range(lrs.shape[0]):\n",
    "        print('Evaluating Learning Rate:', lrs[i])\n",
    "        start_time= time.time()\n",
    "        Weights = sgd(epochs, lrs[i], W, train_set[0], y_train, batch)\n",
    "        end_time = time.time()\n",
    "        valid_acc = test_accuracy(valid_set[0], y_val, Weights)\n",
    "        print('Learning Rate:', lrs[i], 'Validation Accuracy:', valid_acc)\n",
    "        all_valid_acc.append(valid_acc)\n",
    "        if(valid_acc > best_acc):\n",
    "            best_acc = valid_acc\n",
    "            best_lr_test = test_accuracy(test_set[0], y_test, Weights)\n",
    "            best_lr = lrs[i]\n",
    "            run_time = end_time-start_time   \n",
    "        \n",
    "    print('Best Learning Rate:', best_lr)\n",
    "    ##Plot learning rate against validation accuracy\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(\"Validation Accuracy over LR\")\n",
    "    plt.plot(lrs, all_valid_acc)\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.show()\n",
    "    ##test accuracy and run time\n",
    "    print('Test Accuracy:', best_lr_test)\n",
    "    print('Run Time for Training:', run_time)\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Report: \n",
    "- Parameters you chose(batch_size,learning rate,epoch): batch_size = 100, learning rate was a loop through an array, and epoch = 20\n",
    "- Best Learning Rate: 0.01\n",
    "- Test Accuracy: 91.23\n",
    "- Run Time for Training(Using the parameters you chose finally for testing): 4.558103084564209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comparing the Accuracy and Run Times, we have:\n",
    "- Keras: Test Accuracy: 92.19%, Run Time: 17.71 seconds\n",
    "- Theano: Accuracy: 97.86%, Run Time: 2.13 seconds\n",
    "- Numpy: Accuracy: 91.23%, Run Time: 4.56 seconds\n",
    "\n",
    "With Keras, the winning factor (pro) is ease of implementation. It took relatively little time to look at the website, read a few tutorials, and implement it. On the other hand (con), it has a high run time, and it's test-accuracy wasn't as high as I would have hoped. A factor which is both a pro and con, is that you don't really need to understand what's happening at all, in terms of the model – you just add a layer, and trust Keras to do it for you, without appreciating the complexity of what's happening behind the scenes. That is a pro if you just want to implement it quickly, but a con in terms of understanding the depths of what you are doing. \n",
    "\n",
    "With Theano, the level of difficulty stepped up quite a bit, but its performance certainly was impressive. It was not intuitive to implement, even knowing the algorithm – figuring out how to use the symbolic references, and debugging/understanding errors wasn't trivial. On the other hand, it clearly outperformed Keras and Numpy, both in Run Time, and in accuracy. In Theano, you had to understand the algorithm a bit, and implement much of it – it just spared you from figuring out the gradients, and made it a bit higher level. That again is a pro and con, depending on what you are looking for – if you want to just implement it, then that's a con, but if you want to understand precisely what you're doing at what stage, it's mostly a pro (even though you aren't fully implementing the gradient updates). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
