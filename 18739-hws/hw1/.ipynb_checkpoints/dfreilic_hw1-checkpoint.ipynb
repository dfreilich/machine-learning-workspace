{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 18739 Security and Fairness of Deep learning\n",
    "## Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Before You Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.2 Loading in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note: You don't have to use the starter code\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "def load_data(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    try:\n",
    "        #Python3\n",
    "        train_set, valid_set, test_set = pkl.load(f, encoding='latin1')\n",
    "    except:\n",
    "        #Python2\n",
    "        train_set, valid_set, test_set = pkl.load(f)\n",
    "    f.close()\n",
    "    return(train_set,valid_set,test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(50000, 10)\n",
      "(10000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Loading in the data\n",
    "path = 'mnist.pkl.gz' \n",
    "train_set,valid_set,test_set = load_data(path)\n",
    "print(test_set[0].shape)\n",
    "\n",
    "##preprocess the data\n",
    "y_train = np_utils.to_categorical(train_set[1], 10)\n",
    "y_val = np_utils.to_categorical(valid_set[1],10)\n",
    "y_test = np_utils.to_categorical(test_set[1],10)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Level 1: Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 2s - loss: 0.5742 - acc: 0.8539 - val_loss: 0.3639 - val_acc: 0.9042\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.3760 - acc: 0.8960 - val_loss: 0.3260 - val_acc: 0.9111\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.3447 - acc: 0.9037 - val_loss: 0.3074 - val_acc: 0.9169\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.3283 - acc: 0.9082 - val_loss: 0.2990 - val_acc: 0.9183\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.3174 - acc: 0.9116 - val_loss: 0.2931 - val_acc: 0.9185\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.3100 - acc: 0.9128 - val_loss: 0.2889 - val_acc: 0.9194\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.3041 - acc: 0.9143 - val_loss: 0.2853 - val_acc: 0.9196\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.2997 - acc: 0.9159 - val_loss: 0.2804 - val_acc: 0.9224\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.2956 - acc: 0.9175 - val_loss: 0.2804 - val_acc: 0.9215\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.2923 - acc: 0.9188 - val_loss: 0.2755 - val_acc: 0.9230\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.2896 - acc: 0.9189 - val_loss: 0.2749 - val_acc: 0.9233\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.2872 - acc: 0.9203 - val_loss: 0.2731 - val_acc: 0.9237\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.2849 - acc: 0.9208 - val_loss: 0.2714 - val_acc: 0.9241\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.2831 - acc: 0.9211 - val_loss: 0.2706 - val_acc: 0.9249\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.2813 - acc: 0.9216 - val_loss: 0.2702 - val_acc: 0.9239\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.2795 - acc: 0.9219 - val_loss: 0.2689 - val_acc: 0.9250\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.2782 - acc: 0.9224 - val_loss: 0.2685 - val_acc: 0.9260\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.2766 - acc: 0.9229 - val_loss: 0.2683 - val_acc: 0.9249\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.2753 - acc: 0.9240 - val_loss: 0.2679 - val_acc: 0.9258\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.2744 - acc: 0.9232 - val_loss: 0.2659 - val_acc: 0.9261\n",
      "10000/10000 [==============================] - 0s 10us/step\n",
      "[0.27439846390858291, 0.92190000355243684]\n",
      "Run Time for Training: 17.713045120239258\n",
      "Test accuracy: 0.921900003552\n"
     ]
    }
   ],
   "source": [
    "## Start your code here\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import time\n",
    "\n",
    "##Initialize Parameters\n",
    "batch = 100\n",
    "epochs = 20\n",
    "l_r = 0.1\n",
    "features = 784\n",
    "classes = 10\n",
    "\n",
    "##Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "##add a layer to the model, with a specified score/activation function\n",
    "model.add(Dense(10, activation='softmax', input_shape=(features,)))\n",
    "\n",
    "##Compile your model with the desired loss function, optimizer and metrics\n",
    "#make optimizer, to use learning rate\n",
    "sgd = optimizers.SGD(lr=l_r, decay=0.0, momentum=0.0, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "##Fit your training data (You can also specify your validation data using the validation set here)\n",
    "start_time = time.time()\n",
    "model.fit(train_set[0], Y_train, epochs=epochs, batch_size=batch, verbose=2, validation_data=(valid_set[0], y_val))\n",
    "end_time = time.time()\n",
    "\n",
    "# ##Predict on the test data and report the test accuracy(the percentage of images correctly predicted)\n",
    "loss_and_metrics = model.evaluate(test_set[0], y_test, batch_size=batch)\n",
    "print(loss_and_metrics)\n",
    "\n",
    "print('Run Time for Training:', end_time-start_time)\n",
    "print('Test accuracy:', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Report: \n",
    "- Parameters you chose(batch_size,learning rate,epoch): (batch_size=100, learning rate = 0.1, epoch = 20)\n",
    "- Test Accuracy: 0.921900003552\n",
    "- Run Time for Training: 17.713045120239258 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Level 2: Theano Implenmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Accuracy: 0.94882\n",
      "Epoch:  2 Accuracy: 0.96208\n",
      "Epoch:  3 Accuracy: 0.9675\n",
      "Epoch:  4 Accuracy: 0.97042\n",
      "Epoch:  5 Accuracy: 0.97218\n",
      "Epoch:  6 Accuracy: 0.9733\n",
      "Epoch:  7 Accuracy: 0.97422\n",
      "Epoch:  8 Accuracy: 0.97496\n",
      "Epoch:  9 Accuracy: 0.97536\n",
      "Epoch:  10 Accuracy: 0.97588\n",
      "Epoch:  11 Accuracy: 0.97632\n",
      "Epoch:  12 Accuracy: 0.97666\n",
      "Epoch:  13 Accuracy: 0.97686\n",
      "Epoch:  14 Accuracy: 0.97712\n",
      "Epoch:  15 Accuracy: 0.9774\n",
      "Epoch:  16 Accuracy: 0.97768\n",
      "Epoch:  17 Accuracy: 0.97792\n",
      "Epoch:  18 Accuracy: 0.9782\n",
      "Epoch:  19 Accuracy: 0.97832\n",
      "Epoch:  20 Accuracy: 0.9786\n",
      "Run Time for Training 2.0521349906921387\n",
      "Test accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "## Start your code here\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "#Note: You don't have to use the starter code\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_input, n_output):\n",
    "        \"\"\" Initialize the parameters(W,b,p(Y|X)...) of the logistic regression\n",
    "        :X The training/testing set input\n",
    "        :m,n: dimension of training set X\n",
    "        \"\"\"\n",
    "        ## initialize the weights and bias term\n",
    "        self.W = theano.shared(\n",
    "            value=np.random.randn(n_input, n_output),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_output,)),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input\n",
    "        \n",
    "        ## initialize funcs\n",
    "        self.p_y = T.nnet.softmax(T.dot(self.input, self.W) + self.b)\n",
    "        self.pred = T.argmax(self.p_y, axis=1)\n",
    "        self.one_hot = T.extra_ops.to_one_hot(self.pred, n_output)\n",
    "\n",
    "    def score(self, y_true):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "        :y The training set output (ground truth)\n",
    "         \n",
    "        \"\"\"\n",
    "        return T.mean(T.nnet.categorical_crossentropy(self.p_y, y_true))\n",
    "    \n",
    "    def test_accuracy(self, test_y):\n",
    "        \"\"\"Return the test accuracy \n",
    "        :test_y: the testing set output(ground truth)\n",
    "        \"\"\"\n",
    "        ## using self.one_hot, and not self.pred, bc test_y is going to be one_hot encoded as well\n",
    "        return T.mean(T.eq(self.one_hot, test_y))\n",
    "        pass\n",
    "\n",
    "def main():\n",
    "    ##Initialize symbolic variables;\n",
    "    #     np_x_train = np.array(train_set[0])    \n",
    "    x_train_t = theano.shared(np.asarray(train_set[0], dtype=theano.config.floatX))\n",
    "    y_train_t = theano.shared(np.asarray(y_train, dtype=theano.config.floatX))\n",
    "    x_test_t = theano.shared(np.asarray(test_set[0], dtype=theano.config.floatX))\n",
    "    y_test_t = theano.shared(np.asarray(y_test, dtype=theano.config.floatX))\n",
    "\n",
    "    #compute number of minibatches\n",
    "    n_train_batches = x_train_t.get_value(borrow=True).shape[0] // batch\n",
    "    n_test_batches = x_test_t.get_value(borrow=True).shape[0] // batch\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.dmatrix('x')\n",
    "    y = T.dmatrix('y') ## should this be a matrix? We'll see\n",
    "    \n",
    "    ## Create Logistic Regression object and define cost function\n",
    "    LogReg = LogisticRegression(x, features, classes)\n",
    "    cost = LogReg.score(y)\n",
    "\n",
    "    ##Define gradients and update rules;\n",
    "    #this link suggested adding a sum to it: https://groups.google.com/forum/#!msg/theano-users/o1vzlmwVVG8/IjZlwt-GH2AJ\n",
    "    g_W = T.grad(cost=cost, wrt=LogReg.W)\n",
    "    g_b = T.grad(cost=cost, wrt=LogReg.b)\n",
    "    \n",
    "    ##Define your training functions;        \n",
    "    updates = [(LogReg.W, LogReg.W - l_r * g_W),\n",
    "               (LogReg.b, LogReg.b - l_r * g_b)]\n",
    "\n",
    "    train_model = theano.function(\n",
    "              inputs=[index],\n",
    "              outputs=cost,\n",
    "              updates = updates,\n",
    "              givens={\n",
    "                x: x_train_t[index * batch: (index + 1) * batch],\n",
    "                y: y_train_t[index * batch: (index + 1) * batch]\n",
    "            }\n",
    "    )\n",
    "    \n",
    "    test_accuracy = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=LogReg.test_accuracy(y),\n",
    "        givens={\n",
    "            x: x_test_t[index * batch: (index + 1) * batch],\n",
    "            y: y_test_t[index * batch: (index + 1) * batch]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ##Train your model\n",
    "    test_score = 0.\n",
    "    epoch = 0\n",
    "    start_time= time.time()\n",
    "    while (epoch < epochs):\n",
    "        epoch = epoch+1\n",
    "        for batch_index in range(n_train_batches):\n",
    "            avg_cost = train_model(batch_index)\n",
    "        test_losses = [test_accuracy(i)\n",
    "            for i in range(n_test_batches)]\n",
    "        test_score = np.mean(test_losses)\n",
    "        print('Epoch: ', epoch, 'Accuracy:', test_score)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    ##Report your accuracy on the test set.\n",
    "    test_losses = [test_accuracy(i)\n",
    "                   for i in range(n_test_batches)]\n",
    "    test_score = np.mean(test_losses)\n",
    "\n",
    "    print('Run Time for Training', end_time-start_time)\n",
    "    print('Test accuracy:', test_score)\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Report: \n",
    "- Parameters you chose(batch_size,learning rate,epoch): (batch_size=100, learning rate = 0.1, epoch = 20)\n",
    "- Test Accuracy: 0.9786\n",
    "- Run Time for Training: 2.1274356842041016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Level 3: Numpy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start your code here\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Note: You don't have to use the starter code\n",
    "def softmax(o):\n",
    "    \"\"\"\n",
    "    Returns softmax(o) (dim:10 * n)\n",
    "    :o = WX + b,(dim: 10 * n)\n",
    "    \"\"\"\n",
    "    num = np.exp(o-np.max(o))\n",
    "    if num.ndim == 1:\n",
    "        return num / np.sum(num, axis=0)\n",
    "    else:  \n",
    "        return num / np.array([np.sum(num, axis=1)]).T  # ndim = 2\n",
    "\n",
    "def sgd(n_epoch,lr,W,X,y,batch_size=100):\n",
    "    \"\"\"\n",
    "    Returns output: optimal W,b Stochastic Gradient Descent\n",
    "    :n_epoch: Number of epochs\n",
    "    :lr: learning rate\n",
    "    :W: (dim: 10*785)\n",
    "    :X: (dim: n*784)\n",
    "    :y: (dim: 10*n)   \n",
    "    \"\"\"\n",
    "    X = np.c_[np.ones((X.shape[0])), X]\n",
    "    for epoch in range(n_epoch):\n",
    "#         epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        #calculate batch\n",
    "        for (x_batch, y_batch) in next_batch(X,y,batch_size):\n",
    "            ##Compute predictions, softmax(WX_b)\n",
    "            y_prob = softmax(np.dot(x_batch, W.T))\n",
    "            \n",
    "            ##computes derivative of loss wrt W and b\n",
    "            d_y = (y_prob-y_batch)\n",
    "\n",
    "            ##updates weights based on derivative\n",
    "            W -= lr * np.dot(d_y.T, x_batch)\n",
    "            \n",
    "#             epoch_loss.append(cost(y_batch, y_prob))\n",
    "            epoch_acc.append(compute_acc(y_batch, y_prob))\n",
    "#         print('Epoch:', epoch+1, 'Average Accuracy:', np.mean(epoch_acc))\n",
    "    return W\n",
    "\n",
    "def one_hot_encode(seq, num_classes):\n",
    "    num_frames = len(seq)\n",
    "    m = np.zeros((num_frames, num_classes))\n",
    "    m[np.arange(num_frames), seq] = 1\n",
    "    return m\n",
    "\n",
    "def next_batch(X, y, batch_size):\n",
    "     ##loop over our dataset `X` in mini-batches of size `batchSize`\n",
    "    for i in np.arange(0, X.shape[0], batch_size):\n",
    "        ## yield a tuple of the current batched data and labels\n",
    "        yield (X[i:i + batch_size], y[i:i + batch_size])\n",
    "        \n",
    "def cross_entropy(y_true, y_pred):\n",
    "    ent = -np.sum(np.log(y_pred) * (y_true), axis=1)\n",
    "#     print('entropy', ent)\n",
    "    return ent\n",
    "\n",
    "def cost(y_true, y_pred):\n",
    "    c = np.mean(cross_entropy(y_true, y_pred))\n",
    "#     print('cost', c)\n",
    "    return c\n",
    "\n",
    "def compute_acc(y_true, y_prob):\n",
    "    y_pred = np.argmax(y_prob,axis=1)\n",
    "    #transform true labels back to number encoding, to allow more logical equality check\n",
    "    y_true_num = [np.where(r==1)[0][0] for r in y_true]\n",
    "    accuracy = np.sum(y_true_num == y_pred)/y_true.shape[0]\n",
    "    return accuracy * 100\n",
    "    \n",
    "def test_accuracy(X_test,y_test,W):\n",
    "    \"\"\"\n",
    "    Return the accuracy using W,b returned from sgd()\n",
    "    :X_test: (dim: n*785) test dataset input\n",
    "    :y_test: (dim: 10*n) test dataset output\n",
    "    \"\"\"\n",
    "    X_test = np.c_[np.ones((X_test.shape[0])), X_test]\n",
    "    prob = softmax(np.dot(X_test, W.T))\n",
    "    return compute_acc(y_test, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Parameter(Learning Rate) Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Learning Rate: 1e-10\n",
      "Learning Rate: 1e-10 Validation Accuracy: 10.22\n",
      "Evaluating Learning Rate: 1e-09\n",
      "Learning Rate: 1e-09 Validation Accuracy: 10.22\n",
      "Evaluating Learning Rate: 1e-08\n",
      "Learning Rate: 1e-08 Validation Accuracy: 10.22\n",
      "Evaluating Learning Rate: 1e-07\n",
      "Learning Rate: 1e-07 Validation Accuracy: 10.27\n",
      "Evaluating Learning Rate: 1e-06\n",
      "Learning Rate: 1e-06 Validation Accuracy: 13.08\n",
      "Evaluating Learning Rate: 1e-05\n",
      "Learning Rate: 1e-05 Validation Accuracy: 46.84\n",
      "Evaluating Learning Rate: 0.0001\n",
      "Learning Rate: 0.0001 Validation Accuracy: 82.59\n",
      "Evaluating Learning Rate: 0.001\n",
      "Learning Rate: 0.001 Validation Accuracy: 89.82\n",
      "Evaluating Learning Rate: 0.01\n",
      "Learning Rate: 0.01 Validation Accuracy: 91.8\n",
      "Evaluating Learning Rate: 0.1\n",
      "Learning Rate: 0.1 Validation Accuracy: 90.64\n",
      "Evaluating Learning Rate: 1.0\n",
      "Learning Rate: 1.0 Validation Accuracy: 9.91\n",
      "Evaluating Learning Rate: 10.0\n",
      "Learning Rate: 10.0 Validation Accuracy: 9.91\n",
      "Best Learning Rate: 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEjCAYAAAA/ugbCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123379ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.23\n",
      "Run Time for Training: 4.558103084564209\n"
     ]
    }
   ],
   "source": [
    "## Start your code here\n",
    "#Note: You don't have to use the starter code\n",
    "def main():\n",
    "    ##Parameter(Learning Rate) Tuning\n",
    "    #Define Parameters and Variables\n",
    "    lrs = np.power(10,np.arange(-10,2),dtype=float)\n",
    "    W = np.random.randn(10,785)\n",
    "    all_valid_acc = []\n",
    "    best_acc = 0\n",
    "    best_lr = 0\n",
    "    best_lr_test = 0\n",
    "    run_time = 0\n",
    "    \n",
    "    for i in range(lrs.shape[0]):\n",
    "        print('Evaluating Learning Rate:', lrs[i])\n",
    "        start_time= time.time()\n",
    "        Weights = sgd(epochs, lrs[i], W, train_set[0], y_train, batch)\n",
    "        end_time = time.time()\n",
    "        valid_acc = test_accuracy(valid_set[0], y_val, Weights)\n",
    "        print('Learning Rate:', lrs[i], 'Validation Accuracy:', valid_acc)\n",
    "        all_valid_acc.append(valid_acc)\n",
    "        if(valid_acc > best_acc):\n",
    "            best_acc = valid_acc\n",
    "            best_lr_test = test_accuracy(test_set[0], y_test, Weights)\n",
    "            best_lr = lrs[i]\n",
    "            run_time = end_time-start_time   \n",
    "        \n",
    "    print('Best Learning Rate:', best_lr)\n",
    "    ##Plot learning rate against validation accuracy\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(\"Validation Accuracy over LR\")\n",
    "    plt.plot(lrs, all_valid_acc)\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.show()\n",
    "    ##test accuracy and run time\n",
    "    print('Test Accuracy:', best_lr_test)\n",
    "    print('Run Time for Training:', run_time)\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Report: \n",
    "- Parameters you chose(batch_size,learning rate,epoch): batch_size = 100, learning rate was a loop through an array, and epoch = 20\n",
    "- Best Learning Rate: 0.01\n",
    "- Test Accuracy: 91.23\n",
    "- Run Time for Training(Using the parameters you chose finally for testing): 4.558103084564209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comparing the Accuracy and Run Times, we have:\n",
    "- Keras: Test Accuracy: 92.19%, Run Time: 17.71 seconds\n",
    "- Theano: Accuracy: 97.86%, Run Time: 2.13 seconds\n",
    "- Numpy: Accuracy: 91.23%, Run Time: 4.56 seconds\n",
    "\n",
    "With Keras, the winning factor (pro) is ease of implementation. It took relatively little time to look at the website, read a few tutorials, and implement it. On the other hand (con), it has a high run time, and it's test-accuracy wasn't as high as I would have hoped. A factor which is both a pro and con, is that you don't really need to understand what's happening at all, in terms of the model – you just add a layer, and trust Keras to do it for you, without appreciating the complexity of what's happening behind the scenes. That is a pro if you just want to implement it quickly, but a con in terms of understanding the depths of what you are doing. \n",
    "\n",
    "With Theano, the level of difficulty stepped up quite a bit, but its performance certainly was impressive. It was not intuitive to implement, even knowing the algorithm – figuring out how to use the symbolic references, and debugging/understanding errors wasn't trivial. On the other hand, it clearly outperformed Keras and Numpy, both in Run Time, and in accuracy. In Theano, you had to understand the algorithm a bit, and implement much of it – it just spared you from figuring out the gradients, and made it a bit higher level. That again is a pro and con, depending on what you are looking for – if you want to just implement it, then that's a con, but if you want to understand precisely what you're doing at what stage, it's mostly a pro (even though you aren't fully implementing the gradient updates). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
